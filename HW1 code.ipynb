{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\ravin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\ravin\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 20773: expected 15 fields, saw 22\n",
      "Skipping line 39834: expected 15 fields, saw 22\n",
      "Skipping line 52957: expected 15 fields, saw 22\n",
      "Skipping line 54540: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 80276: expected 15 fields, saw 22\n",
      "Skipping line 96168: expected 15 fields, saw 22\n",
      "Skipping line 96866: expected 15 fields, saw 22\n",
      "Skipping line 98175: expected 15 fields, saw 22\n",
      "Skipping line 112539: expected 15 fields, saw 22\n",
      "Skipping line 119377: expected 15 fields, saw 22\n",
      "Skipping line 120065: expected 15 fields, saw 22\n",
      "Skipping line 124703: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 134024: expected 15 fields, saw 22\n",
      "Skipping line 153938: expected 15 fields, saw 22\n",
      "Skipping line 156225: expected 15 fields, saw 22\n",
      "Skipping line 168603: expected 15 fields, saw 22\n",
      "Skipping line 187002: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 200397: expected 15 fields, saw 22\n",
      "Skipping line 203809: expected 15 fields, saw 22\n",
      "Skipping line 207680: expected 15 fields, saw 22\n",
      "Skipping line 223421: expected 15 fields, saw 22\n",
      "Skipping line 244032: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 270329: expected 15 fields, saw 22\n",
      "Skipping line 276484: expected 15 fields, saw 22\n",
      "Skipping line 304755: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 379449: expected 15 fields, saw 22\n",
      "Skipping line 386191: expected 15 fields, saw 22\n",
      "Skipping line 391811: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 414348: expected 15 fields, saw 22\n",
      "Skipping line 414773: expected 15 fields, saw 22\n",
      "Skipping line 417572: expected 15 fields, saw 22\n",
      "Skipping line 419496: expected 15 fields, saw 22\n",
      "Skipping line 430528: expected 15 fields, saw 22\n",
      "Skipping line 442230: expected 15 fields, saw 22\n",
      "Skipping line 450931: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 465377: expected 15 fields, saw 22\n",
      "Skipping line 467685: expected 15 fields, saw 22\n",
      "Skipping line 485055: expected 15 fields, saw 22\n",
      "Skipping line 487220: expected 15 fields, saw 22\n",
      "Skipping line 496076: expected 15 fields, saw 22\n",
      "Skipping line 512269: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 529505: expected 15 fields, saw 22\n",
      "Skipping line 531286: expected 15 fields, saw 22\n",
      "Skipping line 535424: expected 15 fields, saw 22\n",
      "Skipping line 569898: expected 15 fields, saw 22\n",
      "Skipping line 586293: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 593880: expected 15 fields, saw 22\n",
      "Skipping line 599274: expected 15 fields, saw 22\n",
      "Skipping line 607961: expected 15 fields, saw 22\n",
      "Skipping line 612413: expected 15 fields, saw 22\n",
      "Skipping line 615913: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 677580: expected 15 fields, saw 22\n",
      "Skipping line 687191: expected 15 fields, saw 22\n",
      "Skipping line 710819: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 728692: expected 15 fields, saw 22\n",
      "Skipping line 730216: expected 15 fields, saw 22\n",
      "Skipping line 758397: expected 15 fields, saw 22\n",
      "Skipping line 760061: expected 15 fields, saw 22\n",
      "Skipping line 768935: expected 15 fields, saw 22\n",
      "Skipping line 769483: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 822725: expected 15 fields, saw 22\n",
      "Skipping line 823621: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 857041: expected 15 fields, saw 22\n",
      "Skipping line 857320: expected 15 fields, saw 22\n",
      "Skipping line 858565: expected 15 fields, saw 22\n",
      "Skipping line 860629: expected 15 fields, saw 22\n",
      "Skipping line 864033: expected 15 fields, saw 22\n",
      "Skipping line 868673: expected 15 fields, saw 22\n",
      "Skipping line 869189: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 938605: expected 15 fields, saw 22\n",
      "Skipping line 940100: expected 15 fields, saw 22\n",
      "Skipping line 975137: expected 15 fields, saw 22\n",
      "Skipping line 976314: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 985597: expected 15 fields, saw 22\n",
      "Skipping line 990873: expected 15 fields, saw 22\n",
      "Skipping line 991806: expected 15 fields, saw 22\n",
      "Skipping line 1019808: expected 15 fields, saw 22\n",
      "Skipping line 1021526: expected 15 fields, saw 22\n",
      "Skipping line 1023905: expected 15 fields, saw 22\n",
      "Skipping line 1044207: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 1084683: expected 15 fields, saw 22\n",
      "Skipping line 1093288: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 1136430: expected 15 fields, saw 22\n",
      "Skipping line 1139815: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 1179821: expected 15 fields, saw 22\n",
      "Skipping line 1195351: expected 15 fields, saw 22\n",
      "Skipping line 1202007: expected 15 fields, saw 22\n",
      "Skipping line 1224868: expected 15 fields, saw 22\n",
      "Skipping line 1232490: expected 15 fields, saw 22\n",
      "Skipping line 1238697: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 1258654: expected 15 fields, saw 22\n",
      "Skipping line 1279948: expected 15 fields, saw 22\n",
      "Skipping line 1294360: expected 15 fields, saw 22\n",
      "Skipping line 1302240: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 1413654: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 1687095: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 1805966: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: ParserWarning: Skipping line 1892134: expected 15 fields, saw 22\n",
      "\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n",
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\3481158619.py:2: DtypeWarning: Columns (7) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n"
     ]
    }
   ],
   "source": [
    "tsv_file = 'amazon_reviews_us_Office_Products_v1_00.tsv.gz'\n",
    "df_full = pd.read_csv(tsv_file,compression='gzip',sep='\\t',on_bad_lines='warn')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Keep Reviews and Ratings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              Review Rating\n",
      "0                                     Great product.      5\n",
      "1  What's to say about this commodity item except...      5\n",
      "2    Haven't used yet, but I am sure I will like it.      5\n",
      "3  Although this was labeled as &#34;new&#34; the...      1\n",
      "4                    Gorgeous colors and easy to use      4\n",
      "(2640254, 2)\n"
     ]
    }
   ],
   "source": [
    "df = df_full[['review_body','star_rating']].copy()\n",
    "df.rename(columns={'review_body': 'Review', 'star_rating': 'Rating'}, inplace=True)\n",
    "print(df.head())\n",
    "print(df.shape)\n",
    "df['Rating'] = pd.to_numeric(df['Rating'], errors='coerce')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## We form three classes and select 20000 reviews randomly from each class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Negative reviews: 445363\n",
      "Neutral reviews: 193691\n",
      "Positive reviews: 2001183\n",
      "\n",
      "Data shape after discarding rating=3: (2446563, 2)\n"
     ]
    }
   ],
   "source": [
    "count_negative = (df['Rating'] <= 2).sum()\n",
    "count_neutral  = (df['Rating'] == 3).sum()\n",
    "count_positive = (df['Rating'] > 3).sum()\n",
    "print(\"Negative reviews:\", count_negative)\n",
    "print(\"Neutral reviews:\", count_neutral)\n",
    "print(\"Positive reviews:\", count_positive)\n",
    "\n",
    "df = df[df['Rating'] != 3]\n",
    "print(\"\\nData shape after discarding rating=3:\", df.shape)\n",
    "df['Sentiment'] = df['Rating'].apply(lambda x: 1 if x > 3 else 0)\n",
    "\n",
    "df_neg = df[df['Sentiment'] == 0]\n",
    "df_pos = df[df['Sentiment'] == 1]\n",
    "\n",
    "df_neg_sample = df_neg.sample(n=20000, random_state=42)\n",
    "df_pos_sample = df_pos.sample(n=20000, random_state=42)\n",
    "df_downsized = pd.concat([df_neg_sample, df_pos_sample], ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleaning\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length (in characters) before cleaning: 317.63445672283615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ravin\\AppData\\Local\\Temp\\ipykernel_30772\\2816853360.py:11: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  text_no_html = BeautifulSoup(text, \"html.parser\").get_text(separator=\" \")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average length (in characters) after cleaning: 301.5544777238862\n"
     ]
    }
   ],
   "source": [
    "df_downsized.dropna(subset=['Review'], inplace=True)     \n",
    "df_downsized['Review'] = df_downsized['Review'].astype(str)\n",
    "\n",
    "avg_length_before_cleaning = df_downsized['Review'].apply(len).mean()\n",
    "print(\"Average length (in characters) before cleaning:\", avg_length_before_cleaning)\n",
    "\n",
    "df_downsized['Review'] = df_downsized['Review'].str.lower()\n",
    "\n",
    "def remove_html_and_urls(text):\n",
    "    # Remove HTML tags using BeautifulSoup\n",
    "    text_no_html = BeautifulSoup(text, \"html.parser\").get_text(separator=\" \")\n",
    "    \n",
    "    # Remove URLs using regex\n",
    "    # This pattern matches http://, https://, or www. links\n",
    "    text_no_url = re.sub(r'(https?://\\S+|www\\.\\S+)', '', text_no_html)\n",
    "    \n",
    "    return text_no_url\n",
    "\n",
    "df_downsized['Review'] = df_downsized['Review'].apply(remove_html_and_urls)\n",
    "\n",
    "df_downsized['Review'] = df_downsized['Review'].str.replace('[^a-z]', ' ', regex=True)\n",
    "\n",
    "df_downsized['Review'] = df_downsized['Review'].str.split().str.join(' ')\n",
    "\n",
    "#did my best to add as much as possible\n",
    "contractions_dict = {\n",
    "    \"won't\": \"will not\",\n",
    "    \"can't\": \"cannot\",\n",
    "    \"don't\": \"do not\",\n",
    "    \"didn't\": \"did not\",\n",
    "    \"i'm\": \"i am\",\n",
    "    \"it's\": \"it is\",\n",
    "    \"he's\": \"he is\",\n",
    "    \"she's\": \"she is\",\n",
    "    \"that's\": \"that is\",\n",
    "    \"aren't\": \"are not\",\n",
    "    \"weren't\": \"were not\",\n",
    "    \"haven't\": \"have not\",\n",
    "    \"hasn't\": \"has not\",\n",
    "    \"shouldn't\": \"should not\",\n",
    "    \"wouldn't\": \"would not\",\n",
    "    \"couldn't\": \"could not\",\n",
    "    \"isn't\": \"is not\",\n",
    "    \"what's\": \"what is\",\n",
    "    \"where's\": \"where is\",\n",
    "    \"who's\": \"who is\",\n",
    "    \"you'd\": \"you would\",\n",
    "    \"you'll\": \"you will\",\n",
    "    \"you're\": \"you are\",\n",
    "    \"they're\": \"they are\",\n",
    "    \"they've\": \"they have\",\n",
    "    \"we're\": \"we are\",\n",
    "    \"we've\": \"we have\",\n",
    "    \"there's\": \"there is\"\n",
    "}\n",
    "\n",
    "contractions_pattern = re.compile(r'\\b(' + '|'.join(contractions_dict.keys()) + r')\\b')\n",
    "\n",
    "def expand_contractions(text, pattern=contractions_pattern):\n",
    "    def replace(match):\n",
    "        return contractions_dict[match.group(0)]\n",
    "    return pattern.sub(replace, text)\n",
    "\n",
    "df_downsized['Review'] = df_downsized['Review'].apply(expand_contractions)\n",
    "\n",
    "avg_length_after_cleaning = df_downsized['Review'].apply(len).mean()\n",
    "print(\"Average length (in characters) after cleaning:\", avg_length_after_cleaning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SAMPLE REVIEWS BEFORE PREPROCESSING:\n",
      "Review 7516:\n",
      "poor sound quality i exchanged it for the philips id which is a very nice phone system read my review for the philips id\n",
      "--------------------------------------------------------------------------------\n",
      "Review 13706:\n",
      "it burned immediately and i cant return it don t buy\n",
      "--------------------------------------------------------------------------------\n",
      "Review 21103:\n",
      "very cute and unique\n",
      "--------------------------------------------------------------------------------\n",
      "Average length (in characters) before preprocessing: 301.5544777238862\n",
      "SAMPLE REVIEWS AFTER PREPROCESSING:\n",
      "Review 7516:\n",
      "poor sound quality exchanged philip id nice phone system read review philip id\n",
      "--------------------------------------------------------------------------------\n",
      "Review 13706:\n",
      "burned immediately cant return buy\n",
      "--------------------------------------------------------------------------------\n",
      "Review 21103:\n",
      "cute unique\n",
      "--------------------------------------------------------------------------------\n",
      "Average length (in characters) after preprocessing: 186.31031551577578\n",
      "Average length (in characters) before preprocessing: 301.5544777238862\n"
     ]
    }
   ],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def preprocess_text(text):\n",
    "    tokens = text.split()\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    processed_text = \" \".join(tokens)\n",
    "    return processed_text\n",
    "     \n",
    "sample_indices = df_downsized.sample(3, random_state=42).index\n",
    "\n",
    "print(\"SAMPLE REVIEWS BEFORE PREPROCESSING:\")\n",
    "for idx in sample_indices:\n",
    "    print(f\"Review {idx}:\\n{df_downsized.loc[idx, 'Review']}\")\n",
    "    print(\"-\"*80)   \n",
    "\n",
    "avg_length_before_preprocessing = df_downsized['Review'].apply(len).mean()\n",
    "print(\"Average length (in characters) before preprocessing:\", avg_length_before_preprocessing)\n",
    "\n",
    "df_downsized['Review'] = df_downsized['Review'].apply(preprocess_text)\n",
    "\n",
    "print(\"SAMPLE REVIEWS AFTER PREPROCESSING:\")\n",
    "for idx in sample_indices:\n",
    "    print(f\"Review {idx}:\\n{df_downsized.loc[idx, 'Review']}\")\n",
    "    print(\"-\"*80)\n",
    "\n",
    "avg_length_after_preprocessing = df_downsized['Review'].apply(len).mean()\n",
    "print(\"Average length (in characters) after preprocessing:\", avg_length_after_preprocessing)\n",
    "\n",
    "print(\"Average length (in characters) before preprocessing:\", avg_length_before_preprocessing)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (31998, 37291)\n",
      "X_test shape : (8000, 37291)\n",
      "y_train shape: (31998,)\n",
      "y_test shape : (8000,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "#init the vectorizer, used hyperparameters to increase the accuracy\n",
    "vectorizer = TfidfVectorizer(\n",
    "    ngram_range=(1, 2),  \n",
    "    min_df=5,            \n",
    "    max_df=0.8,          \n",
    "    sublinear_tf=True\n",
    ")\n",
    "\n",
    "X = vectorizer.fit_transform(df_downsized['Review'])\n",
    "y = df_downsized['Sentiment'].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape :\", X_test.shape)\n",
    "print(\"y_train shape:\", y_train.shape)\n",
    "print(\"y_test shape :\", y_test.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9910931933245828\n",
      "0.9902107494700088\n",
      "0.992004497470173\n",
      "0.9911068118700659\n",
      "0.860875\n",
      "0.859714928732183\n",
      "0.8616541353383459\n",
      "0.8606834397296282\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Perceptron\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "perceptron_model = Perceptron()#init to train the model\n",
    "perceptron_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = perceptron_model.predict(X_train)\n",
    "y_test_pred = perceptron_model.predict(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "prec_train = precision_score(y_train, y_train_pred)\n",
    "recall_train = recall_score(y_train, y_train_pred)\n",
    "f1_train = f1_score(y_train, y_train_pred)\n",
    "\n",
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "prec_test = precision_score(y_test, y_test_pred)\n",
    "recall_test = recall_score(y_test, y_test_pred)\n",
    "f1_test = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(acc_train)\n",
    "print(prec_train)\n",
    "print(recall_train)\n",
    "print(f1_train)\n",
    "print(acc_test)\n",
    "print(prec_test)\n",
    "print(recall_test)\n",
    "print(f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9595912244515282\n",
      "0.9611431436450238\n",
      "0.9579611468548941\n",
      "0.9595495072735805\n",
      "0.893\n",
      "0.8943130347257172\n",
      "0.8907268170426065\n",
      "0.89251632345555\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "svm_model = SVC(kernel='linear', random_state=42)#init SVM model\n",
    "\n",
    "svm_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = svm_model.predict(X_train)\n",
    "y_test_pred = svm_model.predict(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "prec_train = precision_score(y_train, y_train_pred)\n",
    "recall_train = recall_score(y_train, y_train_pred)\n",
    "f1_train = f1_score(y_train, y_train_pred)\n",
    "\n",
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "prec_test = precision_score(y_test, y_test_pred)\n",
    "recall_test = recall_score(y_test, y_test_pred)\n",
    "f1_test = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(acc_train)\n",
    "print(prec_train)\n",
    "print(recall_train)\n",
    "print(f1_train)\n",
    "print(acc_test)\n",
    "print(prec_test)\n",
    "print(recall_test)\n",
    "print(f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9285267829239328\n",
      "0.9343504684730312\n",
      "0.9219189206071585\n",
      "0.9280930671278101\n",
      "0.884625\n",
      "0.8887198986058301\n",
      "0.8786967418546366\n",
      "0.8836798991808443\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "logreg_model = LogisticRegression(random_state=42) #init logistic regression model\n",
    "\n",
    "logreg_model.fit(X_train, y_train)\n",
    "\n",
    "y_train_pred = logreg_model.predict(X_train)\n",
    "y_test_pred = logreg_model.predict(X_test)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "prec_train = precision_score(y_train, y_train_pred)\n",
    "recall_train = recall_score(y_train, y_train_pred)\n",
    "f1_train = f1_score(y_train, y_train_pred)\n",
    "\n",
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "prec_test = precision_score(y_test, y_test_pred) \n",
    "recall_test = recall_score(y_test, y_test_pred)\n",
    "f1_test = f1_score(y_test, y_test_pred)\n",
    "\n",
    "print(acc_train)\n",
    "print(prec_train)\n",
    "print(recall_train)\n",
    "print(f1_train)\n",
    "print(acc_test)\n",
    "print(prec_test)\n",
    "print(recall_test)\n",
    "print(f1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9127757984874054\n",
      "0.9079126033822985\n",
      "0.9188581422949591\n",
      "0.9133525814162864\n",
      "0.884625\n",
      "0.8887198986058301\n",
      "0.8786967418546366\n",
      "0.8836798991808443\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "nb_model = MultinomialNB() #naive bayes model\n",
    "nb_model.fit(X_train, y_train) \n",
    "\n",
    "y_train_pred = nb_model.predict(X_train)\n",
    "\n",
    "acc_train = accuracy_score(y_train, y_train_pred)\n",
    "prec_train = precision_score(y_train, y_train_pred)\n",
    "recall_train = recall_score(y_train, y_train_pred)\n",
    "f1_train = f1_score(y_train, y_train_pred)\n",
    "\n",
    "acc_test = accuracy_score(y_test, y_test_pred)\n",
    "prec_test = precision_score(y_test, y_test_pred)\n",
    "recall_test = recall_score(y_test, y_test_pred)\n",
    "f1_test = f1_score(y_test, y_test_pred)\n",
    "\n",
    "#training metrics\n",
    "print(acc_train)\n",
    "print(prec_train)\n",
    "print(recall_train)\n",
    "print(f1_train)\n",
    "\n",
    "#testing metrics\n",
    "print(acc_test)\n",
    "print(prec_test)\n",
    "print(recall_test)\n",
    "print(f1_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
